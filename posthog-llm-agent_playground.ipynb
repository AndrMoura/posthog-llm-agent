{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostHog-LLM \n",
    "\n",
    "PostHog-LLM is an extended version of PostHog to work with text analytics and LLM applications. We integrated machine learning models during data ingestion that extract text features from your LLM interactions and we also introduce the ability to visualize your LLM interactions in the insights page!\n",
    "\n",
    "Head over to our [GitHub](https://github.com/postlang/posthog-llm) to quickly deploy your instance locally\n",
    "\n",
    "This demo shows a simple example of how to interact with a chat model and send the interactions to PostHog-LLM for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart notebook\n",
    "!pip install openai posthog -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import posthog\n",
    "\n",
    "posthog.api_key = \"\" # in your PostHog instance under 'Settings' -> 'Project API Key'\n",
    "posthog.host = \"\" # e.g. http://localhost:8000\n",
    "open_ai_key = \"\" # in your OpenAI account under 'API Keys'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent each interaction as task!\n",
    "\n",
    "There's two methods to send your LLM interactions: \n",
    "\n",
    "- linking tasks via a ‚Äú$session_id‚Äù property\n",
    "- incorporating the conversation history in ChatML format within the ‚Äú$llm_input‚Äù property for stateless tasks. (the example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import openai\n",
    "\n",
    "def random_string(length):\n",
    "    letters = string.ascii_letters\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "\n",
    "def chat_loop(client):\n",
    "    user_id = random_string(10)\n",
    "    print(\"Welcome to the Chat with GPT-3.5 Turbo! Type 'quit' to exit.\")\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"You: \")\n",
    "        \n",
    "        if user_message.lower() == \"quit\":\n",
    "            print(\"Exiting the chat. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        print(\"User: \", user_message)\n",
    "        ai_message = response.choices[0].message.content\n",
    "        print(f\"GPT-3.5 Turbo: {ai_message}\")\n",
    "\n",
    "        posthog.capture(\n",
    "            distinct_id=user_id,\n",
    "            event=\"llm-task\",\n",
    "            properties={\n",
    "                '$llm_input': messages,\n",
    "                '$llm_output': ai_message\n",
    "                # timestamp is automatically added :)\n",
    "                # Add any other properties you want to track\n",
    "            }\n",
    "        )\n",
    "\n",
    "        messages.append({\"role\": \"system\", \"content\": ai_message})\n",
    "\n",
    "\n",
    "client = openai.OpenAI(api_key=open_ai_key)\n",
    "chat_loop(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your Activity feed in your PostHog instance to visualize the interactions you just tracked! ü§ó"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
